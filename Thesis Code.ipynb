{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0ff76e-b21c-4aed-9f69-7a00c85d8fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTARGET WORDS in DiaWUG:\\n    \\namarrar/atar\\nargolla\\nbanco\\nbaúl/maletero\\nbolo\\nbotar\\ncartera/bolso\\nchamaco/pibe/chico\\nchurro\\ncoche/carro\\nflete\\nfranela\\ngato\\nguagua/colectivo\\nplomero/fontanero\\npollera/falda\\nsaco\\nsindicar/acusar\\ntinto\\nvaina\\nvereda\\nvidriera/escaparate\\nvolante/timón\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALL TARGET WORDS in DiaWUG:\n",
    "    \n",
    "amarrar/atar\n",
    "argolla\n",
    "banco\n",
    "baúl/maletero\n",
    "bolo\n",
    "botar\n",
    "cartera/bolso\n",
    "chamaco/pibe/chico\n",
    "churro\n",
    "coche/carro\n",
    "flete\n",
    "franela\n",
    "gato\n",
    "guagua/colectivo\n",
    "plomero/fontanero\n",
    "pollera/falda\n",
    "saco\n",
    "sindicar/acusar\n",
    "tinto\n",
    "vaina\n",
    "vereda\n",
    "vidriera/escaparate\n",
    "volante/timón\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3cc6f-9229-404a-9b49-c5d452b67653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3462844-868e-4ac8-90d2-4da2ad4e349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the occurences in Spanish disambiguation pages\n",
    "\n",
    "def search_disambiguation(path, target_words) -> None:\n",
    "    \"\"\"\n",
    "    A search function that will print keys that occur in the disambiguation pages.\n",
    "    \"\"\"\n",
    "    # reads the file\n",
    "    with open(path, \"r\") as file:\n",
    "        disambig = json.load(file)\n",
    "        # for every key in the target words list if there is a match in disambig pages the match will be printed\n",
    "        for key in target_words:\n",
    "            if key in disambig:\n",
    "                print(f\"'{key}' is in disambiguation pages\")\n",
    "\n",
    "                \n",
    "# path to disambiguation pages\n",
    "path = \"data/wiki/eswiki-20220301/disambig.json\"\n",
    "\n",
    "# added _(desambiguación) after all the target words because some pages are only listed with that addition\n",
    "target_words = [\n",
    "    \"Amarrar\", \"Atar\", \"Argolla\", \"Banco\", \"Baúl\", \"Maletero\", \"Bolo\", \"Botar\", \"Cartera\", \"Bolso\", \"Chamaco\", \"Pibe\", \"Chico\", \n",
    "    \"Churro\", \"Coche\", \"Carro\", \"Flete\", \"Franela\", \"Gato\", \"Guagua\", \"Colectivo\", \"Plomero\", \"Fontanero\", \"Pollera\", \"Falda\", \n",
    "    \"Saco\", \"Sindicar\", \"Acusar\", \"Tinto\", \"Vaina\", \"Vereda\", \"Vidriera\", \"Escaparate\", \"Volante\", \"Timón\",\n",
    "    \"Amarrar_(desambiguación)\", \"Atar_(desambiguación)\", \"Argolla_(desambiguación)\", \n",
    "    \"Banco_(desambiguación)\", \"Baúl_(desambiguación)\", \"Maletero_(desambiguación)\", \n",
    "    \"Bolo_(desambiguación)\", \"Botar_(desambiguación)\", \"Cartera_(desambiguación)\", \"Bolso_(desambiguación)\", \n",
    "    \"Chamaco_(desambiguación)\", \"Pibe_(desambiguación)\", \"Chico_(desambiguación)\", \n",
    "    \"Churro_(desambiguación)\", \"Coche_(desambiguación)\", \"Carro_(desambiguación)\", \n",
    "    \"Flete_(desambiguación)\", \"Franela_(desambiguación)\", \"Gato_(desambiguación)\", \"Guagua_(desambiguación)\", \n",
    "    \"Colectivo_(desambiguación)\", \"Plomero_(desambiguación)\", \"Fontanero_(desambiguación)\", \n",
    "    \"Pollera_(desambiguación)\", \"Falda_(desambiguación)\", \n",
    "    \"Saco_(desambiguación)\", \"Sindicar_(desambiguación)\", \"Acusar_(desambiguación)\", \n",
    "    \"Tinto_(desambiguación)\", \"Vaina_(desambiguación)\", \"Vereda_(desambiguación)\", \n",
    "    \"Vidriera_(desambiguación)\", \"Escaparate_(desambiguación)\", \"Volante_(desambiguación)\", \n",
    "    \"Timón_(desambiguación)\",\n",
    "    ]\n",
    "\n",
    "search_disambiguation(path, target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8ac68-e922-4d33-94dc-84c94ea9d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the occurences of the top 25% most referenced words in our dataset\n",
    "\n",
    "def search_quarter(path, target_words) -> None:\n",
    "    \"\"\"\n",
    "    A search function that will print keys that occur in the 25% most frequently referenced pages.\n",
    "    \"\"\"\n",
    "    # reads the file\n",
    "    with open(path, \"r\") as file:\n",
    "        quarter = json.load(file)\n",
    "        # for every key in the target words list if there is a match in q0.25 data the match will be printed\n",
    "        for key in target_words:\n",
    "            if key in quarter:\n",
    "                print(f\"'{key}' is in q0.25 data\")\n",
    "\n",
    "    \n",
    "# path to q0.25 data file\n",
    "path = \"data/wiki/eswiki-20220301/experiments/clean-q0.25.json\"\n",
    "\n",
    "# target words from DIAWUG\n",
    "target_words = [\n",
    "    \"amarrar\", \"atar\", \"argolla\", \"banco\", \"baúl\", \"maletero\", \"bolo\", \"botar\", \"cartera\", \"bolso\", \"chamaco\", \"pibe\", \"chico\", \n",
    "    \"churro\", \"coche\", \"carro\", \"flete\", \"franela\", \"gato\", \"guagua\", \"colectivo\", \"plomero\", \"fontanero\", \"pollera\", \"falda\", \n",
    "    \"saco\", \"sindicar\", \"acusar\", \"tinto\", \"vaina\", \"vereda\", \"vidriera\", \"escaparate\", \"volante\", \"timón\"\n",
    "    ]\n",
    "\n",
    "search_quarter(path, target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4c64d-f2af-474c-a2c4-89f314202b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the occurences in entire Spanish dataset\n",
    "\n",
    "def search_full(path, target_words) -> None:\n",
    "    \"\"\"\n",
    "    A search function that will print keys that occur in the entire data.\n",
    "    \"\"\"\n",
    "    # reads the file\n",
    "    with open(path, \"r\") as file:\n",
    "        full = json.load(file)\n",
    "        # for every key in the target words list if there is a match in the dataset the match will be printed\n",
    "        for key in target_words:\n",
    "            if key in full:\n",
    "                print(f\"'{key}' is in full dataset\")\n",
    "\n",
    "    \n",
    "# path to q1 data file\n",
    "path = \"data/wiki/eswiki-20220301/experiments/clean-q1.json\"\n",
    "\n",
    "# target words from DIAWUG\n",
    "target_words = [\n",
    "    \"amarrar\", \"atar\", \"argolla\", \"banco\", \"baúl\", \"maletero\", \"bolo\", \"botar\", \"cartera\", \"bolso\", \"chamaco\", \"pibe\", \"chico\", \n",
    "    \"churro\", \"coche\", \"carro\", \"flete\", \"franela\", \"gato\", \"guagua\", \"colectivo\", \"plomero\", \"fontanero\", \"pollera\", \"falda\", \n",
    "    \"saco\", \"sindicar\", \"acusar\", \"tinto\", \"vaina\", \"vereda\", \"vidriera\", \"escaparate\", \"volante\", \"timón\"\n",
    "    ]\n",
    "\n",
    "search_full(path, target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059534f-db1e-4abe-b26e-6742ebf88739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For finding wikidata IDs on our dataset's items\n",
    "\n",
    "# choose path file\n",
    "path = 'data/wiki/eswiki-20220301/experiments/clean-q1.json'\n",
    "\n",
    "# read the chosen file \n",
    "with open(path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "# write the word you would like to find wiki IDs for\n",
    "# do not capitalise any letters\n",
    "data[\"vaina\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f3d02-31b8-43f5-b2d8-03f748d981b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For finding wikidata IDs on our disambiguation items\n",
    "\n",
    "# choose path file\n",
    "path = 'data/wiki/eswiki-20220301/disambig.json'\n",
    "\n",
    "# read the chosen file \n",
    "with open(path, 'r') as json_file:\n",
    "    disambig = json.load(json_file)\n",
    "    \n",
    "# if not found try to add the word with \"_(desambiguación)\" after it just in case\n",
    "# and capitalise all words\n",
    "disambig[\"Colectivo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82868cf8-acba-4c99-8c58-e6bcb75cd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some notable examples of vehicle words\n",
    "# disambig['Guagua']\n",
    "# disambig['Coche']\n",
    "# disambig['Auto']\n",
    "# disambig['Carro_(desambiguación)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbf485-cb70-418d-b300-326383364ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values in the DIAWUG csv file are poorly distributed with both the ID and cluster being in the same column\n",
    "# Therefore this code splits both things into a dictionary that will be used in the next code block\n",
    "# In hindsight my mistake was coding it for csv files instead of tsv files but it works nonetheless\n",
    "\n",
    "def columns_to_dict(path, column) -> dict[str,int]:\n",
    "    \"\"\"\n",
    "    Splits one column containing ID and cluster into dictionary items.\n",
    "    \"\"\"\n",
    "    # The empty dictionary\n",
    "    dictionary = {}\n",
    "    \n",
    "    # reads the path file\n",
    "    with open(path, \"r\", newline=\"\") as file:\n",
    "        file = csv.DictReader(file)\n",
    "        # iterates through the rows in the file and splits on \",\"\n",
    "        for row in file:\n",
    "            values = row[column].split(\",\")\n",
    "            # iterates through each string and removes the tabs, white-space, etc. \n",
    "            for value in values:\n",
    "                key = value.strip()[0:-1]\n",
    "                key = key.replace(\"\\t\",\"\")\n",
    "                val = value.strip()[-1]\n",
    "                # all unique keys are added to our dictionary with their matching values\n",
    "                if key not in dictionary:\n",
    "                    dictionary[key] = val\n",
    "    return dictionary\n",
    "\n",
    "# path to csv file\n",
    "path = \"data/diawug/clusters/csv usable word versions/churro.csv\"\n",
    "\n",
    "# column name, should have been a tsv file which is why \"\\t\" is in between both values\n",
    "column = \"identifier\\tcluster\"\n",
    "\n",
    "# we store the results in variable \"cluster_identifier\"\n",
    "cluster_identifier = columns_to_dict(path, column)\n",
    "# print(cluster_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870a1538-addc-45aa-a7a6-416df6e9a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the previous dictionary values the identifiers are linked with their clusters \n",
    "# So the \"uses.tsv\" file can be used to create data equivalent to that of Mewsli \n",
    "# Now we will work with tsv files since we learned from the last code block that it is easier\n",
    "\n",
    "def add_cluster_column(path, output_file, ID_column, cluster_identifier) -> None:\n",
    "    \"\"\"\n",
    "    Adds one extra column to the uses.tsv file provided by DIAWUG by checking the identifier and adding\n",
    "    the corresponding cluster to each identifier by using the dictionary we created earlier.\n",
    "    \"\"\"\n",
    "    # reads ONE of the word uses.tsv files (specify in path) and creates the output file that will be written\n",
    "    with open(path, \"r\", newline=\"\") as input_file, open(output_file, \"w\", newline=\"\") as output_file:\n",
    "        # name our files and this time we use tab as our delimiter for tsv files\n",
    "        uses = csv.reader(input_file, delimiter=\"\\t\")\n",
    "        output = csv.writer(output_file, delimiter=\"\\t\")\n",
    "        # we take the first row as our column names\n",
    "        columns = next(uses)\n",
    "        # we add our new column named \"cluster\"\n",
    "        columns.append(\"cluster\")\n",
    "        # we write our first row in the output file with all the desired column names\n",
    "        output.writerow(columns)\n",
    "        \n",
    "        # iterates through the rows in uses file\n",
    "        for row in uses:\n",
    "            # we indicate that the ID is in the fourth column\n",
    "            ID = row[4]\n",
    "            # if the ID is in our dictionary we append the value (cluster) of that ID to the row\n",
    "            if ID in cluster_identifier:\n",
    "                row.append(cluster_identifier[ID])\n",
    "            # otherwise we append nothing to the row\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "            # then we write the appended value for each row to the output file\n",
    "            output.writerow(row)\n",
    "            \n",
    "# path to word uses file of choice\n",
    "path = \"data/diawug/data/churro/uses.tsv\"\n",
    "\n",
    "# name of output file\n",
    "output_file = \"with_clusters.tsv\"\n",
    "\n",
    "# name of our target column\n",
    "ID_column = \"identifier\"\n",
    "\n",
    "add_cluster_column(input_file,output_file,column,cluster_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c50c0-6ef3-4e76-b307-335525acc8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second to last step is to add one more column to the Mewsli-like file by referencing the tsv file we just made above \n",
    "# that added the cluster numbers with the DIAWUG database we made separately with the wikidata IDs by cluster and word\n",
    "# this code will add the \"wikidata\" column to the tsv file with clusters that we made in the last cell\n",
    "\n",
    "def diawug_to_mewsli(cluster_file, reference_file, output_file, \n",
    "                     cluster_column1, cluster_column2, reference_column1, \n",
    "                     reference_column2, wiki_column) -> None:\n",
    "    \"\"\"\n",
    "    Adds the \"Wikidata\" column to the uses file with clusters from the last cell.\n",
    "    By checking that two separate columns match this time \"word\" and \"cluster\".\n",
    "    This way the code adds the correct wikidata ID for each word cluster.\n",
    "    \"\"\"\n",
    "    # empty dictionary to store all identifier and cluster combinations with their wikidata IDs\n",
    "    reference_dict = {}\n",
    "    \n",
    "    # reads the reference file (DIAWUG)\n",
    "    with open(reference_file, \"r\", newline= \"\") as reference_f:\n",
    "        references = csv.DictReader(reference_f, delimiter = \"\\t\")\n",
    "        # for each row stores relevant combinations in our dictionary with their corresponding wikidata IDs\n",
    "        reference_dict = {(row[reference_column1], row[reference_column2]):row[wiki_column] for row in references}\n",
    "\n",
    "    # reads the cluster file and creates output mewsli file for writing\n",
    "    with open(cluster_file, \"r\", newline=\"\") as cluster_f, open(output_file, \"w\", newline=\"\") as mewsli:\n",
    "        clusters = csv.DictReader(cluster_f, delimiter= \"\\t\")\n",
    "        output = csv.writer(mewsli, delimiter=\"\\t\")\n",
    "        # uses fieldnames to get column names in cluster file with the addition of the wikidata column name\n",
    "        columns = clusters.fieldnames + [wiki_column]\n",
    "        # writes the column names in the first row\n",
    "        output.writerow(columns)\n",
    "        # iterates through the rows in cluster file and compares the keys to their corresponding reference dictionary keys\n",
    "        for row in clusters:\n",
    "            # the key equivalent in cluster file is a combination of \"lemma\" and \"cluster\"\n",
    "            key = (row[cluster_column1], row[cluster_column2])\n",
    "            # we assign the values of our reference_dict from all the matching keys to the wikidata column, if there is none then we assign \"{}\"\n",
    "            row[wiki_column] = reference_dict.get(key,\"{}\")\n",
    "            # writes all the rows in the output file with the corresponding values from each column\n",
    "            output.writerow([row[column] for column in columns])\n",
    "\n",
    "# All file variables and target column names\n",
    "cluster_file = \"with_clusters.tsv\"\n",
    "reference_file = \"Target Words in DiaWUG final - for code use.tsv\"\n",
    "output_file = \"mewsli.tsv\"\n",
    "cluster_column1 = \"lemma\"\n",
    "cluster_column2 = \"cluster\"\n",
    "reference_column1 = \"word\"\n",
    "reference_column2 = \"cluster\"\n",
    "wiki_column = \"wikidata\"\n",
    "\n",
    "diawug_to_mewsli(cluster_file,reference_file,output_file,cluster_column1,cluster_column2,reference_column1,reference_column2,wiki_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade5ef7-bf07-44d9-8f40-d230814f9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last step now that all the data is in the same tsv file is to \n",
    "# remove the unnecessary columns from the tsv file\n",
    "# and also the first row as the header is not needed for mewsli\n",
    "\n",
    "def trimmed_mewsli(input_file, output_file, column_index) -> None:\n",
    "    \"\"\"\n",
    "    Trims the columns in the Mewsli-like file that we created in the cell above\n",
    "    into the three columns that we need to emalute the evaluation file.\n",
    "    \"\"\"\n",
    "    # reads the input mewsli file and opens the output file\n",
    "    with open(input_file, \"r\", newline=\"\") as mewsli_f, open(output_file,\"w\", newline=\"\") as output_f:\n",
    "        mewsli = csv.reader(mewsli_f, delimiter=\"\\t\")\n",
    "        trimmed = csv.writer(output_f, delimiter=\"\\t\")\n",
    "        # skips first row because we don't want column names in the file\n",
    "        next(mewsli)\n",
    "        # iterates through the rows and stores only the items in specified columns\n",
    "        for row in mewsli:\n",
    "            extracted_row = [row[index] for index in column_index]\n",
    "            # writes the extracted row values in each column\n",
    "            trimmed.writerow(extracted_row)\n",
    "\n",
    "# name of mewsli file from above\n",
    "input_file = \"mewsli.tsv\"\n",
    "\n",
    "# name of output file \n",
    "output_file = \"volante_timón.tsv\"\n",
    "\n",
    "# index of the columns we want to keep\n",
    "column_index = [4,10,6]\n",
    "\n",
    "trimmed_mewsli(input_file,output_file,column_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d02ab9-8fe6-4d59-9e4b-b44c06aaaab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Which Mewsli files are complete?\n",
    "\n",
    "banco \n",
    "bolo \n",
    "churro \n",
    "flete \n",
    "saco \n",
    "tinto \n",
    "vaina \n",
    "vereda \n",
    "gato \n",
    "cartera_bolso\n",
    "chamaco_pibe_chico\n",
    "coche_carro\n",
    "pollera_falda\n",
    "guagua_colectivo\n",
    "plomero_fontanero\n",
    "\n",
    "misses many values:\n",
    "\n",
    "volante_timón\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
